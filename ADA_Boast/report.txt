AdaBoost Text Classification Report
Model Evaluation and Performance Analysis

here we train the model based on the 1k sentence of each language .

1. Introduction
This report presents the evaluation results of an AdaBoost-based text classification model trained on TF-IDF features.
The model was tuned using GridSearchCV to find the best hyperparameters and was evaluated on test and development datasets
using classification metrics such as precision, recall, and F1-score.

Best Parameters: {'learning_rate': 1.0, 'n_estimators': 200}

Test Set Classification Report:

  
              precision    recall  f1-score   support

         asm       0.00      0.00      0.00        62
         ben       1.00      0.60      0.75        90
         eng       1.00      0.54      0.71        90
         guj       1.00      0.62      0.77       146
         hin       1.00      0.33      0.50        94
         mal       0.75      0.39      0.51       197
         ori       1.00      0.44      0.61        73
         pan       1.00      0.39      0.56       102
         tam       1.00      0.56      0.72       108
         tel       0.28      1.00      0.44       210
         urd       1.00      0.59      0.74        73

    accuracy                           0.55      1245
   macro avg       0.82      0.50      0.57      1245
weighted avg       0.79      0.55      0.58      1245


Dev Set Classification Report:

  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
              precision    recall  f1-score   support

         asm       0.00      0.00      0.00        61
         ben       1.00      0.52      0.68        89
         eng       1.00      0.67      0.80        90
         guj       1.00      0.61      0.76       145
         hin       1.00      0.37      0.54        93
         mal       0.69      0.31      0.42       196
         ori       1.00      0.40      0.57        72
         pan       1.00      0.33      0.50       102
         tam       1.00      0.58      0.73       107
         tel       0.28      1.00      0.44       209
         urd       1.00      0.71      0.83        72

    accuracy                           0.54      1236
   macro avg       0.82      0.50      0.57      1236
weighted avg       0.78      0.54      0.56      1236

Evaluation Metrics (Macro, Micro, Weighted):


Macro Averaging:
Dev Precision: 0.8154720077423157 Dev Recall: 0.4987286085986383 Dev F1-Score: 0.5701560266464131
Test Precision: 0.8213739274331201 Test Recall: 0.4970613717803833 Test F1-Score: 0.5732660003320877

Micro Averaging:
Dev Precision: 0.5444983818770227 Dev Recall: 0.5444983818770227 Dev F1-Score: 0.5444983818770227
Test Precision: 0.5518072289156627 Test Recall: 0.5518072289156627 Test F1-Score: 0.5518072289156627


Weighted Averaging:
Dev Precision: 0.7797772075713626 Dev Recall: 0.5444983818770227 Dev F1-Score: 0.5640952394988015
