AdaBoost Text Classification Report
Model Evaluation and Performance Analysis

here we train the model based on the 1k sentence of each language .

1. Introduction
This report presents the evaluation results of an AdaBoost-based text classification model trained on TF-IDF features.
The model was tuned using GridSearchCV to find the best hyperparameters and was evaluated on test and development datasets
using classification metrics such as precision, recall, and F1-score.

2. Best Hyperparameters Found
The following hyperparameters were selected after performing grid search:

n_estimators: 50
learning_rate: 1.0
3. Evaluation Metrics
The performance of the model was assessed using different averaging techniques:

Macro Averaging – Treats all classes equally
Micro Averaging – Sums true positives, false positives, and false negatives across classes
Weighted Averaging – Accounts for class imbalance


The detailed classification reports for the test and development datasets are presented below.

4. Test Set Classification Report


              precision    recall  f1-score   support

         asm       1.00      0.24      0.39        62
         ben       1.00      0.64      0.78        90
         eng       1.00      0.63      0.78        90
         guj       1.00      0.62      0.77       146
         hin       1.00      0.47      0.64        94
         mal       1.00      0.31      0.47       197
         ori       1.00      0.44      0.61        73
         pan       1.00      0.39      0.56       102
         tam       1.00      0.56      0.72       108
         tel       0.29      1.00      0.44       210
         urd       1.00      0.68      0.81        73

    accuracy                           0.58      1245
   macro avg       0.94      0.55      0.63      1245
weighted avg       0.88      0.58      0.61      1245


5. Dev Set Classification Report
              precision    recall  f1-score   support

         asm       1.00      0.13      0.23        61
         ben       0.98      0.61      0.75        89
         eng       1.00      0.77      0.87        90
         guj       1.00      0.61      0.76       145
         hin       1.00      0.48      0.65        93
         mal       1.00      0.23      0.38       196
         ori       1.00      0.47      0.64        72
         pan       1.00      0.33      0.50       102
         tam       1.00      0.58      0.73       107
         tel       0.28      1.00      0.44       209
         urd       1.00      0.78      0.88        72

    accuracy                           0.57      1236
   macro avg       0.93      0.54      0.62      1236
weighted avg       0.88      0.57      0.59      1236


Evaluation Metrics (Macro, Micro, Weighted):

Macro Averaging:
Dev Precision: 0.9331484360147173 Dev Recall: 0.5447990693973507 Dev F1-Score: 0.6207889190995343
Test Precision: 0.9350296442687747 Test Recall: 0.5455445507371047 Test F1-Score: 0.6344696896220444

Micro Averaging:
Dev Precision: 0.5703883495145631 Dev Recall: 0.5703883495145631 Dev F1-Score: 0.5703883495145631
Test Precision: 0.5775100401606426 Test Recall: 0.5775100401606426 Test F1-Score: 0.5775100401606426

Weighted Averaging:
Dev Precision: 0.8774191525725349 Dev Recall: 0.5703883495145631 Dev F1-Score: 0.5942978690211285
Test Precision: 0.8794525929806181 Test Recall: 0.5775100401606426 Test F1-Score: 0.6122219308741138
